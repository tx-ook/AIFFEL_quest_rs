{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2927630e-38c0-46be-9c6c-4b4948ee1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch and torchvision\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d50169-1ac8-4b9c-880d-dec412a7323c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 확인\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c536ab0-4746-4192-a4a5-8f21ce7493b8",
   "metadata": {},
   "source": [
    "## Stanford_dogs dataset을 활용한 Augmentation 효과 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e85bb-942f-4fee-bc92-cdf4f4d52242",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "dataset_dir = \"~/work/data_augmentation/data/Images/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # 크기 통일\n",
    "    transforms.ToTensor(),  # Tensor 변환\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 정규화 추가\n",
    "])\n",
    "full_dataset = ImageFolder(root=dataset_dir, transform=transform)\n",
    "\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.583 * total_size)  # 약 12,000개\n",
    "test_size = total_size - train_size   # 약 8,580개\n",
    "ds_train, ds_test = random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(ds_test, batch_size=32, shuffle=False)\n",
    "ds_info = {\n",
    "    \"num_classes\": len(full_dataset.classes),\n",
    "    \"class_names\": full_dataset.classes\n",
    "}\n",
    "\n",
    "print(\"=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884a460-775b-4539-bb82-7093d1284207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader에서 일부 배치 가져오기\n",
    "def show_examples(data_loader, class_names, num_images=6):\n",
    "    data_iter = iter(data_loader)\n",
    "    images, labels = next(data_iter)\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n",
    "    for i in range(num_images):\n",
    "        image = images[i].permute(1, 2, 0).numpy()  # (C, H, W) → (H, W, C)\n",
    "        image = (image * 0.5) + 0.5\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(class_names[labels[i].item()])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# 훈련 데이터 샘플 시각화\n",
    "show_examples(train_loader, ds_info[\"class_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f69d5-4b29-4ea7-9ecd-a9315a2f2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_resize_img():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # 크기 통일\n",
    "        transforms.ToTensor(),  # Tensor 변환\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 정규화 추가\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8632266-c09e-4f42-91c6-92cf719da1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment():\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ColorJitter(brightness=0.2)\n",
    "    ])\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329d89c8-56d4-4cda-af69-dee04aed581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 가공하는 메인함수\n",
    "def apply_normalize_on_dataset(dataset, is_test=False, batch_size=16, with_aug=False):\n",
    "    transform = normalize_and_resize_img()\n",
    "\n",
    "    if not is_test and with_aug:\n",
    "        dataset.dataset.transform = transforms.Compose([\n",
    "            *augment().transforms,\n",
    "            *transform.transforms\n",
    "        ])\n",
    "    else:\n",
    "        dataset.dataset.transform = transform\n",
    "\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=not is_test, num_workers=2, pin_memory=True)\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9a3869-5688-41a5-a8ff-728e5dc0f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def augment2():\n",
    "    return transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # 좌우 반전\n",
    "        transforms.RandomVerticalFlip(p=0.5),    # 상하 반전\n",
    "        transforms.RandomRotation(degrees=(0, 90, 180, 270)),  # 90도 단위 회전\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # 밝기, 대비, 색상 조정\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # 랜덤 크롭 후 리사이즈\n",
    "        transforms.Lambda(lambda img: torch.clamp(img, 0, 1))  # 0~1 값으로 클리핑\n",
    "    ])\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68505b-3504-4c40-abcb-8e0b7d43debf",
   "metadata": {},
   "source": [
    "## 비교실험하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d514560f-5ca8-4f29-be7c-151580598074",
   "metadata": {},
   "source": [
    "### Pre-trained ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a83dbb1-e266-40ec-92ad-6509fe0c7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "num_classes = len(ds_info[\"class_names\"])\n",
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    resnet50,\n",
    "    nn.AdaptiveAvgPool2d((1, 1)),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(resnet50.fc.in_features, num_classes),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "print(\"=3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddadc14-c4f8-4193-b2fc-99fb91e0d757",
   "metadata": {},
   "source": [
    "### Pre-trained ResNet50 + augmentatin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f52e2-1804-4a3b-851a-b1dec2263bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "aug_resnet50.fc = nn.Linear(aug_resnet50.fc.in_features, num_classes)\n",
    "\n",
    "aug_resnet50 = nn.Sequential(\n",
    "    aug_resnet50,\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "print(\"=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779810f-03a6-41c8-877f-1da004a88378",
   "metadata": {},
   "source": [
    "### augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f8a9f-6b46-4bcc-bd8b-57d3a0e0dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train_no_aug = apply_normalize_on_dataset(ds_train, with_aug=False)\n",
    "ds_train_aug = apply_normalize_on_dataset(ds_train, with_aug=True)\n",
    "ds_test = apply_normalize_on_dataset(ds_test, is_test=True)\n",
    "\n",
    "print(\"=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fac6ed-77c9-48ae-ad9a-4d2beb5c31bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52344f29-11a8-44af-bb97-c60fead345d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import torch.optim as optim\n",
    "\n",
    "#EPOCH = 20  # Augentation 적용 효과를 확인하기 위해 필요한 epoch 수\n",
    "EPOCH = 3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(resnet50.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, train_loader, test_loader, epochs):\n",
    "    model.to(device)\n",
    "    history = {'val_accuracy': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_acc = 100. * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_acc = 100. * correct / total\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        print(f\"Validation Accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "history_resnet50_no_aug = train(resnet50, ds_train_no_aug, ds_test, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e47a8d8-abef-4ef2-bcf7-1014f101c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history_resnet50_aug = train(aug_resnet50, ds_train_aug, ds_test, EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a9ff7-0cd5-4db2-bca8-154166a074fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_resnet50_no_aug['val_accuracy'], 'r', label='No Augmentation')\n",
    "plt.plot(history_resnet50_aug['val_accuracy'], 'b', label='With Augmentation')\n",
    "plt.title('Model validation accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df59ccd1-59f9-4906-9f3a-e3b3a8df6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_resnet50_no_aug['val_accuracy'], 'r', label='No Augmentation')\n",
    "plt.plot(history_resnet50_aug['val_accuracy'], 'b', label='With Augmentation')\n",
    "plt.title('Model validation accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.ylim(0.50, 0.80)    # 출력하고자 하는  Accuracy 범위를 지정해 주세요.\n",
    "#plt.ylim(0.72, 0.76)  # EPOCH=20으로 진행한다면 이 범위가 적당합니다.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92095521-22da-42a1-98c1-0f5009b04902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18660787-06d2-4e60-ae6c-5efdf5c34bdd",
   "metadata": {},
   "source": [
    "# 심화 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d4098-3c20-438a-856c-d1f0424e6a46",
   "metadata": {},
   "source": [
    "#### CutMix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d90b63-0823-4185-8f7b-920641dc2d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_box(image_a, image_b):\n",
    "    # image.shape = (channel, height, width)\n",
    "    image_size_x = image_a.shape[2]  # Width\n",
    "    image_size_y = image_a.shape[1]  # Height\n",
    "\n",
    "    # Get center of box\n",
    "    x = torch.randint(0, image_size_x, (1,)).item()\n",
    "    y = torch.randint(0, image_size_y, (1,)).item()\n",
    "\n",
    "    width = max(1, int(image_size_x * torch.sqrt(1 - torch.rand(1)).item()))\n",
    "    height = max(1, int(image_size_y * torch.sqrt(1 - torch.rand(1)).item()))\n",
    "\n",
    "    # Clip box in image and get minmax bbox\n",
    "    x_min = max(0, x - width // 2)\n",
    "    y_min = max(0, y - height // 2)\n",
    "    x_max = min(image_size_x, x + width // 2 + 1)\n",
    "    y_max = min(image_size_y, y + height // 2 + 1)\n",
    "\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "x_min, y_min, x_max, y_max = get_clip_box(image_a, image_b)\n",
    "\n",
    "print('x :', x_min, x_max)\n",
    "print('y :', y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151e9a3d-60ee-45ce-8bd9-d83025aeac3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def mix_2_images(image_a, image_b, x_min, y_min, x_max, y_max):\n",
    "    # image.shape = (C, H, W)\n",
    "    image_size_x = image_a.shape[2]  # Width\n",
    "    image_size_y = image_a.shape[1]  # Height\n",
    "\n",
    "    if isinstance(image_a, np.ndarray):\n",
    "        image_a = torch.from_numpy(image_a).to(device)\n",
    "    if isinstance(image_b, np.ndarray):\n",
    "        image_b = torch.from_numpy(image_b).to(device)\n",
    "\n",
    "    top = image_a[:, :y_min, :]\n",
    "    middle_left = image_a[:, y_min:y_max, :x_min]\n",
    "    middle_center = image_b[:, y_min:y_max, x_min:x_max]\n",
    "    middle_right = image_a[:, y_min:y_max, x_max:]\n",
    "    bottom = image_a[:, y_max:, :]\n",
    "\n",
    "    top = top.to(device)\n",
    "    middle_left = middle_left.to(device)\n",
    "    middle_center = middle_center.to(device)\n",
    "    middle_right = middle_right.to(device)\n",
    "    bottom = bottom.to(device)\n",
    "\n",
    "    # 중간 부분(왼쪽, 중앙, 오른쪽) 결합\n",
    "    middle = torch.cat([middle_left, middle_center, middle_right], dim=2)\n",
    "\n",
    "    # 전체 이미지 결합 (위 + 중간 + 아래)\n",
    "    mixed_img = torch.cat([top, middle, bottom], dim=1)\n",
    "\n",
    "    return mixed_img\n",
    "\n",
    "mixed_img = mix_2_images(image_a, image_b, x_min, y_min, x_max, y_max)\n",
    "\n",
    "plt.imshow(mixed_img.cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a108c2fd-3fda-4b89-bb0a-755dcf5b1328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# mix two labels\n",
    "def mix_2_labels(label_a, label_b, x_min, y_min, x_max, y_max, num_classes=120):\n",
    "    image_size_x = image_a.shape[2]  # Width\n",
    "    image_size_y = image_a.shape[1]  # Height\n",
    "\n",
    "    # 바운딩 박스 비율 계산\n",
    "    mixed_area = (x_max - x_min) * (y_max - y_min)\n",
    "    total_area = image_size_x * image_size_y\n",
    "    ratio = mixed_area / total_area\n",
    "\n",
    "     # 원-핫 벡터 변환\n",
    "    if isinstance(label_a, int):\n",
    "        label_a = F.one_hot(torch.tensor(label_a), num_classes=num_classes).float()\n",
    "    if isinstance(label_b, int):\n",
    "        label_b = F.one_hot(torch.tensor(label_b), num_classes=num_classes).float()\n",
    "\n",
    "    # 비율에 따라 라벨 혼합\n",
    "    mixed_label = (1 - ratio) * label_a + ratio * label_b\n",
    "    return mixed_label\n",
    "\n",
    "# 예제 실행\n",
    "mixed_label = mix_2_labels(label_a, label_b, x_min, y_min, x_max, y_max)\n",
    "print(mixed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f904772-1838-436e-a772-6a705330a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix(images, labels, prob=1.0, batch_size=16, img_size=224, num_classes=120):\n",
    "    mixed_imgs = []\n",
    "    mixed_labels = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image_a = images[i]\n",
    "        label_a = labels[i]\n",
    "\n",
    "        j = torch.randint(0, batch_size, (1,)).item()  # 다른 샘플 선택\n",
    "        image_b = images[j]\n",
    "        label_b = labels[j]\n",
    "\n",
    "        # 바운딩 박스 생성\n",
    "        x_min, y_min, x_max, y_max = get_clip_box(image_a, image_b)\n",
    "\n",
    "        # 이미지 섞기\n",
    "        mixed_imgs.append(mix_2_images(image_a, image_b, x_min, y_min, x_max, y_max))\n",
    "        mixed_labels.append(mix_2_labels(label_a, label_b, x_min, y_min, x_max, y_max, num_classes))\n",
    "\n",
    "    # 텐서 변환\n",
    "    mixed_imgs = torch.stack(mixed_imgs).reshape(batch_size, 3, img_size, img_size)\n",
    "    mixed_labels = torch.stack(mixed_labels).reshape(batch_size, num_classes)\n",
    "\n",
    "    return mixed_imgs, mixed_labels\n",
    "\n",
    "print(\"=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3621bad-099b-4322-8013-4073f26adc65",
   "metadata": {},
   "source": [
    "#### Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823eecf-17ca-49e4-8e22-68004d46cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for mixup\n",
    "def mixup_2_images(image_a, image_b, label_a, label_b, num_classes=120):\n",
    "    ratio = torch.rand(1).item()  # 0~1 사이의 랜덤 값\n",
    "\n",
    "    # 라벨 원핫 인코딩\n",
    "    if isinstance(label_a, int):\n",
    "        label_a = F.one_hot(torch.tensor(label_a), num_classes=num_classes).float()\n",
    "    if isinstance(label_b, int):\n",
    "        label_b = F.one_hot(torch.tensor(label_b), num_classes=num_classes).float()\n",
    "\n",
    "    # 이미지와 라벨 혼합\n",
    "    mixed_image = (1 - ratio) * image_a + ratio * image_b\n",
    "    mixed_label = (1 - ratio) * label_a + ratio * label_b\n",
    "\n",
    "    return mixed_image, mixed_label\n",
    "\n",
    "# 예제 실행\n",
    "mixed_img, mixed_label = mixup_2_images(image_a, image_b, label_a, label_b)\n",
    "\n",
    "plt.imshow(mixed_img)\n",
    "plt.show()\n",
    "\n",
    "print(mixed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcff745-98d5-4153-895a-42ceb408a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup(images, labels, batch_size=16, img_size=224, num_classes=120):\n",
    "    mixed_imgs = []\n",
    "    mixed_labels = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image_a = images[i]\n",
    "        label_a = labels[i]\n",
    "\n",
    "        # 랜덤하게 다른 이미지 선택\n",
    "        j = torch.randint(0, batch_size, (1,)).item()\n",
    "        image_b = images[j]\n",
    "        label_b = labels[j]\n",
    "\n",
    "        # Mixup 적용\n",
    "        mixed_img, mixed_label = mixup_2_images(image_a, image_b, label_a, label_b, num_classes)\n",
    "\n",
    "        mixed_imgs.append(mixed_img)\n",
    "        mixed_labels.append(mixed_label)\n",
    "\n",
    "    # 배치 차원 추가\n",
    "    mixed_imgs = torch.stack(mixed_imgs).view(batch_size, 3, img_size, img_size)  # (B, C, H, W)\n",
    "    mixed_labels = torch.stack(mixed_labels).view(batch_size, num_classes)  # (B, num_classes)\n",
    "\n",
    "    return mixed_imgs, mixed_labels\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d5f74-9e80-4821-9158-5415853c2393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b579c47-882e-4c46-86f6-741f9582a5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b43ba98-b702-4a64-a639-50e997698956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
